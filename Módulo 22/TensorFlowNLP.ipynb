{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'I love my dog', \n",
    "    'I love my cat',\n",
    "    'You love my dog',\n",
    "    'Do you think my dog is amazing'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
      "[[ 0  0  0  5  3  2  4]\n",
      " [ 0  0  0  5  3  2  7]\n",
      " [ 0  0  0  6  3  2  4]\n",
      " [ 8  6  9  2  4 10 11]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences)\n",
    "print(word_index)\n",
    "print(sequences)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "datastore = []\n",
    "with open(\"sarcasm.json\", \"r\") as f:\n",
    "    for line in f:\n",
    "        datastore.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'article_link': 'https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5',\n",
       "  'headline': \"former versace store clerk sues over secret 'black code' for minority shoppers\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/roseanne-revival-review_us_5ab3a497e4b054d118e04365',\n",
       "  'headline': \"the 'roseanne' revival catches up to our thorny political mood, for better and worse\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/mom-starting-to-fear-son-s-web-series-closest-thing-she-1819576697',\n",
       "  'headline': \"mom starting to fear son's web series closest thing she will have to grandchild\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/boehner-just-wants-wife-to-listen-not-come-up-with-alt-1819574302',\n",
       "  'headline': 'boehner just wants wife to listen, not come up with alternative debt-reduction ideas',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/jk-rowling-wishes-snape-happy-birthday_us_569117c4e4b0cad15e64fdcb',\n",
       "  'headline': 'j.k. rowling wishes snape happy birthday in the most magical way',\n",
       "  'is_sarcastic': 0}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datastore[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "urls = []\n",
    "for item in datastore:\n",
    "    sentences.append(item['headline'])\n",
    "    labels.append(item['is_sarcastic'])\n",
    "    urls.append(item['article_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"former versace store clerk sues over secret 'black code' for minority shoppers\",\n",
       " \"the 'roseanne' revival catches up to our thorny political mood, for better and worse\",\n",
       " \"mom starting to fear son's web series closest thing she will have to grandchild\",\n",
       " 'boehner just wants wife to listen, not come up with alternative debt-reduction ideas',\n",
       " 'j.k. rowling wishes snape happy birthday in the most magical way']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 100\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "training_size = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data for training and test\n",
    "training_sentences = sentences[0:training_size]\n",
    "training_labels = labels[0:training_size]\n",
    "\n",
    "testing_sentences =sentences[training_size:]\n",
    "testing_labels = labels[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(training_labels)\n",
    "testing_padded = np.array(testing_padded)\n",
    "testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "625/625 - 1s - loss: 0.6626 - accuracy: 0.5964 - val_loss: 0.5757 - val_accuracy: 0.7733 - 1s/epoch - 2ms/step\n",
      "Epoch 2/30\n",
      "625/625 - 1s - loss: 0.4362 - accuracy: 0.8328 - val_loss: 0.3887 - val_accuracy: 0.8386 - 880ms/epoch - 1ms/step\n",
      "Epoch 3/30\n",
      "625/625 - 1s - loss: 0.3178 - accuracy: 0.8720 - val_loss: 0.3544 - val_accuracy: 0.8520 - 861ms/epoch - 1ms/step\n",
      "Epoch 4/30\n",
      "625/625 - 1s - loss: 0.2658 - accuracy: 0.8965 - val_loss: 0.3565 - val_accuracy: 0.8410 - 857ms/epoch - 1ms/step\n",
      "Epoch 5/30\n",
      "625/625 - 1s - loss: 0.2300 - accuracy: 0.9120 - val_loss: 0.3486 - val_accuracy: 0.8496 - 852ms/epoch - 1ms/step\n",
      "Epoch 6/30\n",
      "625/625 - 1s - loss: 0.2023 - accuracy: 0.9230 - val_loss: 0.3516 - val_accuracy: 0.8505 - 844ms/epoch - 1ms/step\n",
      "Epoch 7/30\n",
      "625/625 - 1s - loss: 0.1795 - accuracy: 0.9332 - val_loss: 0.3702 - val_accuracy: 0.8498 - 849ms/epoch - 1ms/step\n",
      "Epoch 8/30\n",
      "625/625 - 1s - loss: 0.1619 - accuracy: 0.9416 - val_loss: 0.3745 - val_accuracy: 0.8514 - 849ms/epoch - 1ms/step\n",
      "Epoch 9/30\n",
      "625/625 - 1s - loss: 0.1444 - accuracy: 0.9474 - val_loss: 0.3888 - val_accuracy: 0.8527 - 843ms/epoch - 1ms/step\n",
      "Epoch 10/30\n",
      "625/625 - 1s - loss: 0.1322 - accuracy: 0.9536 - val_loss: 0.4126 - val_accuracy: 0.8481 - 851ms/epoch - 1ms/step\n",
      "Epoch 11/30\n",
      "625/625 - 1s - loss: 0.1200 - accuracy: 0.9585 - val_loss: 0.4295 - val_accuracy: 0.8492 - 875ms/epoch - 1ms/step\n",
      "Epoch 12/30\n",
      "625/625 - 1s - loss: 0.1091 - accuracy: 0.9622 - val_loss: 0.4741 - val_accuracy: 0.8378 - 850ms/epoch - 1ms/step\n",
      "Epoch 13/30\n",
      "625/625 - 1s - loss: 0.0996 - accuracy: 0.9664 - val_loss: 0.4771 - val_accuracy: 0.8430 - 849ms/epoch - 1ms/step\n",
      "Epoch 14/30\n",
      "625/625 - 1s - loss: 0.0921 - accuracy: 0.9685 - val_loss: 0.5042 - val_accuracy: 0.8438 - 868ms/epoch - 1ms/step\n",
      "Epoch 15/30\n",
      "625/625 - 1s - loss: 0.0843 - accuracy: 0.9723 - val_loss: 0.5355 - val_accuracy: 0.8395 - 873ms/epoch - 1ms/step\n",
      "Epoch 16/30\n",
      "625/625 - 1s - loss: 0.0776 - accuracy: 0.9753 - val_loss: 0.5794 - val_accuracy: 0.8351 - 848ms/epoch - 1ms/step\n",
      "Epoch 17/30\n",
      "625/625 - 1s - loss: 0.0720 - accuracy: 0.9768 - val_loss: 0.5905 - val_accuracy: 0.8365 - 851ms/epoch - 1ms/step\n",
      "Epoch 18/30\n",
      "625/625 - 1s - loss: 0.0649 - accuracy: 0.9797 - val_loss: 0.6214 - val_accuracy: 0.8325 - 844ms/epoch - 1ms/step\n",
      "Epoch 19/30\n",
      "625/625 - 1s - loss: 0.0606 - accuracy: 0.9821 - val_loss: 0.6570 - val_accuracy: 0.8317 - 848ms/epoch - 1ms/step\n",
      "Epoch 20/30\n",
      "625/625 - 1s - loss: 0.0558 - accuracy: 0.9828 - val_loss: 0.6999 - val_accuracy: 0.8283 - 843ms/epoch - 1ms/step\n",
      "Epoch 21/30\n",
      "625/625 - 1s - loss: 0.0504 - accuracy: 0.9844 - val_loss: 0.7275 - val_accuracy: 0.8284 - 854ms/epoch - 1ms/step\n",
      "Epoch 22/30\n",
      "625/625 - 1s - loss: 0.0462 - accuracy: 0.9877 - val_loss: 0.8040 - val_accuracy: 0.8198 - 854ms/epoch - 1ms/step\n",
      "Epoch 23/30\n",
      "625/625 - 1s - loss: 0.0425 - accuracy: 0.9883 - val_loss: 0.8121 - val_accuracy: 0.8208 - 847ms/epoch - 1ms/step\n",
      "Epoch 24/30\n",
      "625/625 - 1s - loss: 0.0407 - accuracy: 0.9873 - val_loss: 0.8338 - val_accuracy: 0.8214 - 837ms/epoch - 1ms/step\n",
      "Epoch 25/30\n",
      "625/625 - 1s - loss: 0.0369 - accuracy: 0.9898 - val_loss: 0.9457 - val_accuracy: 0.8182 - 853ms/epoch - 1ms/step\n",
      "Epoch 26/30\n",
      "625/625 - 1s - loss: 0.0354 - accuracy: 0.9893 - val_loss: 0.9087 - val_accuracy: 0.8199 - 849ms/epoch - 1ms/step\n",
      "Epoch 27/30\n",
      "625/625 - 1s - loss: 0.0322 - accuracy: 0.9902 - val_loss: 0.9393 - val_accuracy: 0.8198 - 844ms/epoch - 1ms/step\n",
      "Epoch 28/30\n",
      "625/625 - 1s - loss: 0.0279 - accuracy: 0.9926 - val_loss: 1.0078 - val_accuracy: 0.8155 - 850ms/epoch - 1ms/step\n",
      "Epoch 29/30\n",
      "625/625 - 1s - loss: 0.0255 - accuracy: 0.9936 - val_loss: 1.0825 - val_accuracy: 0.8153 - 843ms/epoch - 1ms/step\n",
      "Epoch 30/30\n",
      "625/625 - 1s - loss: 0.0274 - accuracy: 0.9918 - val_loss: 1.1241 - val_accuracy: 0.8132 - 846ms/epoch - 1ms/step\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n",
      "[[7.497187e-01]\n",
      " [6.173562e-07]]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\n",
    "    'granny starting to fear spiders in the garden might be real',\n",
    "    'the weather today is bright and sunny'\n",
    "]\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "print(model.predict(padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
